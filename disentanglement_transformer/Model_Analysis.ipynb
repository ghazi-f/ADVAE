{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Experiments\\GLUE_BENCH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length:  8.900745464443235  Quantiles .25, 0.5, 0.7, and 0.9 : [ 7.  8. 10. 13. 14. 15.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length:  8.920423872838148  Quantiles .25, 0.5, 0.7, and 0.9 : [ 7.  9. 10. 13. 14. 15.]\nMean length:  8.920423872838148  Quantiles .25, 0.5, 0.7, and 0.9 : [ 7.  9. 10. 13. 14. 15.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  11895 , On device:  cuda\nLoss Type:  VAE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at step 25936\nUnsupervised training examples:  90112\nNumber of parameters:  03.77 M\nInference parameters:  02.21 M\nGeneration parameters:  03.08 M\nEmbedding parameters:  01.52 M\n"
     ]
    }
   ],
   "source": [
    "# This file will implement the main training loop for a model\n",
    "# Model 1\n",
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "print(os.getcwd())\n",
    "sys.path.append('../..')\n",
    "# os.chdir('../..')\n",
    "\n",
    "from torch import device\n",
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from data_prep import NLIGenData2 as Data\n",
    "from disentanglement_transformer.models import DisentanglementTransformerVAE as Model\n",
    "from disentanglement_transformer.h_params import DefaultTransformerHParams as HParams\n",
    "from disentanglement_transformer.graphs import *\n",
    "from disentanglement_transformer.graphs import get_structured_auto_regressive_disentanglement_graph\n",
    "from components.criteria import *\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Training and Optimization\n",
    "parser.add_argument(\"--test_name\", default='nliLM/StructuredAutoreg5', type=str)\n",
    "parser.add_argument(\"--max_len\", default=17, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--grad_accu\", default=1, type=int)\n",
    "parser.add_argument(\"--n_epochs\", default=10000, type=int)\n",
    "parser.add_argument(\"--test_freq\", default=32, type=int)\n",
    "parser.add_argument(\"--complete_test_freq\", default=160, type=int)\n",
    "parser.add_argument(\"--generation_weight\", default=1, type=float)\n",
    "parser.add_argument(\"--device\", default='cuda:0', choices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cpu\"], type=str)\n",
    "parser.add_argument(\"--embedding_dim\", default=128, type=int)#################\"\n",
    "parser.add_argument(\"--z_size\", default=768, type=int)#################\"\n",
    "parser.add_argument(\"--n_latents\", default=[16, 16, 16], type=int)#################\"\n",
    "parser.add_argument(\"--text_rep_l\", default=2, type=int)\n",
    "parser.add_argument(\"--text_rep_h\", default=768, type=int)\n",
    "parser.add_argument(\"--encoder_h\", default=768, type=int)#################\"\n",
    "parser.add_argument(\"--encoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--decoder_h\", default=768, type=int)\n",
    "parser.add_argument(\"--decoder_l\", default=3, type=int)#################\"\n",
    "parser.add_argument(\"--highway\", default=False, type=bool)\n",
    "parser.add_argument(\"--markovian\", default=True, type=bool)\n",
    "parser.add_argument(\"--losses\", default='VAE', choices=[\"VAE\", \"IWAE\"], type=str)\n",
    "parser.add_argument(\"--training_iw_samples\", default=5, type=int)\n",
    "parser.add_argument(\"--testing_iw_samples\", default=20, type=int)\n",
    "parser.add_argument(\"--test_prior_samples\", default=10, type=int)\n",
    "parser.add_argument(\"--anneal_kl0\", default=2000, type=int)\n",
    "parser.add_argument(\"--anneal_kl1\", default=4000, type=int)\n",
    "parser.add_argument(\"--grad_clip\", default=100., type=float)\n",
    "parser.add_argument(\"--kl_th\", default=0/(768*3), type=float or None)\n",
    "parser.add_argument(\"--dropout\", default=0.0, type=float)\n",
    "parser.add_argument(\"--word_dropout\", default=.0, type=float)\n",
    "parser.add_argument(\"--l2_reg\", default=0, type=float)\n",
    "parser.add_argument(\"--lr\", default=2e-4, type=float)\n",
    "parser.add_argument(\"--lr_reduction\", default=4., type=float)\n",
    "parser.add_argument(\"--wait_epochs\", default=3, type=float)\n",
    "parser.add_argument(\"--save_all\", default=True, type=bool)\n",
    "\n",
    "flags, _ = parser.parse_known_args()\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "MAX_LEN = flags.max_len\n",
    "BATCH_SIZE = flags.batch_size\n",
    "GRAD_ACCU = flags.grad_accu\n",
    "N_EPOCHS = flags.n_epochs\n",
    "TEST_FREQ = flags.test_freq\n",
    "COMPLETE_TEST_FREQ = flags.complete_test_freq\n",
    "DEVICE = device(flags.device)\n",
    "# This prevents illegal memory access on multigpu machines (unresolved issue on torch's github)\n",
    "if flags.device.startswith('cuda'):\n",
    "    torch.cuda.set_device(int(flags.device[-1]))\n",
    "LOSSES = {'IWAE': [IWLBo],\n",
    "          'VAE': [ELBo]}[flags.losses]\n",
    "#  LOSSES = [IWLBo]\n",
    "ANNEAL_KL = [flags.anneal_kl0*flags.grad_accu, flags.anneal_kl1*flags.grad_accu]\n",
    "LOSS_PARAMS = [1]\n",
    "if flags.grad_accu > 1:\n",
    "    LOSS_PARAMS = [w/flags.grad_accu for w in LOSS_PARAMS]\n",
    "\n",
    "data = Data(MAX_LEN, BATCH_SIZE, N_EPOCHS, DEVICE, False)\n",
    "h_params = HParams(len(data.vocab.itos), len(data.tags.itos), MAX_LEN, BATCH_SIZE, N_EPOCHS,\n",
    "                   device=DEVICE, vocab_ignore_index=data.vocab.stoi['<pad>'], decoder_h=flags.decoder_h,\n",
    "                   decoder_l=flags.decoder_l, encoder_h=flags.encoder_h, encoder_l=flags.encoder_l,\n",
    "                   text_rep_h=flags.text_rep_h, text_rep_l=flags.text_rep_l,\n",
    "                   test_name=flags.test_name, grad_accumulation_steps=GRAD_ACCU,\n",
    "                   optimizer_kwargs={'lr': flags.lr, #'weight_decay': flags.l2_reg, 't0':100, 'lambd':0.},\n",
    "                                     'weight_decay': flags.l2_reg, 'betas': (0.9, 0.85)},\n",
    "                   is_weighted=[], graph_generator=get_structured_auto_regressive_disentanglement_graph,\n",
    "                   z_size=flags.z_size, embedding_dim=flags.embedding_dim, anneal_kl=ANNEAL_KL,\n",
    "                   grad_clip=flags.grad_clip*flags.grad_accu, kl_th=flags.kl_th, highway=flags.highway,\n",
    "                   losses=LOSSES, dropout=flags.dropout, training_iw_samples=flags.training_iw_samples,\n",
    "                   testing_iw_samples=flags.testing_iw_samples, loss_params=LOSS_PARAMS, optimizer=optim.AdamW,\n",
    "                   markovian=flags.markovian, word_dropout=flags.word_dropout, contiguous_lm=False,\n",
    "                   test_prior_samples=flags.test_prior_samples, n_latents=flags.n_latents)\n",
    "val_iterator = iter(data.val_iter)\n",
    "print(\"Words: \", len(data.vocab.itos), \", On device: \", DEVICE.type)\n",
    "print(\"Loss Type: \", flags.losses)\n",
    "model = Model(data.vocab, data.tags, h_params, wvs=data.wvs)\n",
    "if DEVICE.type == 'cuda':\n",
    "    model.cuda(DEVICE)\n",
    "\n",
    "total_unsupervised_train_samples = len(data.train_iter)*BATCH_SIZE\n",
    "print(\"Unsupervised training examples: \", total_unsupervised_train_samples)\n",
    "current_time = time()\n",
    "#print(model)\n",
    "number_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.infer_bn.parameters() if p.requires_grad)\n",
    "print(\"Inference parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.gen_bn.parameters() if p.requires_grad)\n",
    "print(\"Generation parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.word_embeddings.parameters() if p.requires_grad)\n",
    "print(\"Embedding parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Experiments\\GLUE_BENCH\\tb_logs\\nliLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length:  8.900745464443235  Quantiles .25, 0.5, 0.7, and 0.9 : [ 7.  8. 10. 13. 14. 15.]\nMean length:  8.920423872838148  Quantiles .25, 0.5, 0.7, and 0.9 : [ 7.  9. 10. 13. 14. 15.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length:  8.920423872838148  Quantiles .25, 0.5, 0.7, and 0.9 : [ 7.  9. 10. 13. 14. 15.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  11895 , On device:  cuda\nLoss Type:  VAE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at step 39396\nUnsupervised training examples:  90048\nNumber of parameters:  07.75 M\nInference parameters:  05.48 M\nGeneration parameters:  05.84 M\nEmbedding parameters:  03.57 M\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "print(os.getcwd())\n",
    "#sys.path.append('../..')\n",
    "os.chdir('../..')\n",
    "\n",
    "from torch import device\n",
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from data_prep import NLIGenData2 as Data\n",
    "from disentanglement_transformer.models import DisentanglementTransformerVAE as Model\n",
    "from disentanglement_transformer.h_params import DefaultTransformerHParams as HParams\n",
    "from disentanglement_transformer.graphs import *\n",
    "from components.criteria import *\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Training and Optimization\n",
    "k, kz =1, 10\n",
    "parser.add_argument(\"--test_name\", default='unnamed', type=str)\n",
    "parser.add_argument(\"--max_len\", default=20, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=512, type=int)\n",
    "parser.add_argument(\"--grad_accu\", default=1, type=int)\n",
    "parser.add_argument(\"--n_epochs\", default=10000, type=int)\n",
    "parser.add_argument(\"--test_freq\", default=32, type=int)\n",
    "parser.add_argument(\"--complete_test_freq\", default=32, type=int)\n",
    "parser.add_argument(\"--generation_weight\", default=1, type=float)\n",
    "parser.add_argument(\"--device\", default='cuda:0', choices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cpu\"], type=str)\n",
    "parser.add_argument(\"--embedding_dim\", default=300, type=int)#################\"\n",
    "parser.add_argument(\"--pretrained_embeddings\", default=True, type=bool)#################\"\n",
    "parser.add_argument(\"--z_size\", default=768*kz, type=int)#################\"\n",
    "parser.add_argument(\"--z_emb_dim\", default=768*k, type=int)#################\"\n",
    "parser.add_argument(\"--n_latents\", default=[16, 16, 16], type=list)#################\"\n",
    "parser.add_argument(\"--text_rep_l\", default=2, type=int)\n",
    "parser.add_argument(\"--text_rep_h\", default=768*k, type=int)\n",
    "parser.add_argument(\"--encoder_h\", default=768*k, type=int)#################\"\n",
    "parser.add_argument(\"--encoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--decoder_h\", default=768*k, type=int)\n",
    "parser.add_argument(\"--decoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--highway\", default=False, type=bool)\n",
    "parser.add_argument(\"--markovian\", default=True, type=bool)\n",
    "parser.add_argument(\"--losses\", default='VAE', choices=[\"VAE\", \"IWAE\"], type=str)\n",
    "parser.add_argument(\"--graph\", default='Discrete', choices=[\"Discrete\", \"Normal\"], type=str)\n",
    "parser.add_argument(\"--training_iw_samples\", default=5, type=int)\n",
    "parser.add_argument(\"--testing_iw_samples\", default=20, type=int)\n",
    "parser.add_argument(\"--test_prior_samples\", default=10, type=int)\n",
    "parser.add_argument(\"--anneal_kl0\", default=3000, type=int)\n",
    "parser.add_argument(\"--anneal_kl1\", default=6000, type=int)\n",
    "parser.add_argument(\"--grad_clip\", default=10., type=float)\n",
    "parser.add_argument(\"--kl_th\", default=0*12/(1536*28/16), type=float or None)\n",
    "parser.add_argument(\"--dropout\", default=0.0, type=float)\n",
    "parser.add_argument(\"--word_dropout\", default=.0, type=float)\n",
    "parser.add_argument(\"--l2_reg\", default=0, type=float)\n",
    "parser.add_argument(\"--lr\", default=2e-4, type=float)\n",
    "parser.add_argument(\"--lr_reduction\", default=4., type=float)\n",
    "parser.add_argument(\"--wait_epochs\", default=3, type=float)\n",
    "parser.add_argument(\"--save_all\", default=True, type=bool)\n",
    "\n",
    "flags, _ = parser.parse_known_args()\n",
    "\n",
    "# Manual Settings, Deactivate before pushing\n",
    "if True:\n",
    "    flags.losses = 'VAE'\n",
    "    flags.batch_size = 64\n",
    "    flags.grad_accu = 1\n",
    "    flags.max_len = 17\n",
    "    flags.test_name = \"nliLM/Discrete3\"\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "GRAPH = {\"Discrete\": get_discrete_auto_regressive_disentanglement_graph,\n",
    "         \"Normal\": get_structured_auto_regressive_disentanglement_graph}[flags.graph]\n",
    "MAX_LEN = flags.max_len\n",
    "BATCH_SIZE = flags.batch_size\n",
    "MAS_ELBO = 5\n",
    "GRAD_ACCU = flags.grad_accu\n",
    "N_EPOCHS = flags.n_epochs\n",
    "TEST_FREQ = flags.test_freq\n",
    "COMPLETE_TEST_FREQ = flags.complete_test_freq\n",
    "DEVICE = device(flags.device)\n",
    "# This prevents illegal memory access on multigpu machines (unresolved issue on torch's github)\n",
    "if flags.device.startswith('cuda'):\n",
    "    torch.cuda.set_device(int(flags.device[-1]))\n",
    "LOSSES = {'IWAE': [IWLBo],\n",
    "          'VAE': [ELBo]}[flags.losses]\n",
    "#  LOSSES = [IWLBo]\n",
    "ANNEAL_KL = [flags.anneal_kl0*flags.grad_accu, flags.anneal_kl1*flags.grad_accu]\n",
    "LOSS_PARAMS = [1]\n",
    "if flags.grad_accu > 1:\n",
    "    LOSS_PARAMS = [w/flags.grad_accu for w in LOSS_PARAMS]\n",
    "\n",
    "data = Data(MAX_LEN, BATCH_SIZE, N_EPOCHS, DEVICE, flags.pretrained_embeddings)\n",
    "h_params = HParams(len(data.vocab.itos), len(data.tags.itos), MAX_LEN, BATCH_SIZE, N_EPOCHS,\n",
    "                   device=DEVICE, vocab_ignore_index=data.vocab.stoi['<pad>'], decoder_h=flags.decoder_h,\n",
    "                   decoder_l=flags.decoder_l, encoder_h=flags.encoder_h, encoder_l=flags.encoder_l,\n",
    "                   text_rep_h=flags.text_rep_h, text_rep_l=flags.text_rep_l,\n",
    "                   test_name=flags.test_name, grad_accumulation_steps=GRAD_ACCU,\n",
    "                   optimizer_kwargs={'lr': flags.lr, #'weight_decay': flags.l2_reg, 't0':100, 'lambd':0.},\n",
    "                                     'weight_decay': flags.l2_reg, 'betas': (0.9, 0.85)},\n",
    "                   is_weighted=[], graph_generator=GRAPH,\n",
    "                   z_size=flags.z_size, embedding_dim=flags.embedding_dim, anneal_kl=ANNEAL_KL,\n",
    "                   grad_clip=flags.grad_clip*flags.grad_accu, kl_th=flags.kl_th, highway=flags.highway,\n",
    "                   losses=LOSSES, dropout=flags.dropout, training_iw_samples=flags.training_iw_samples,\n",
    "                   testing_iw_samples=flags.testing_iw_samples, loss_params=LOSS_PARAMS, optimizer=optim.AdamW,\n",
    "                   markovian=flags.markovian, word_dropout=flags.word_dropout, contiguous_lm=False,\n",
    "                   test_prior_samples=flags.test_prior_samples, n_latents=flags.n_latents, max_elbo=5,\n",
    "                   z_emb_dim=flags.z_emb_dim)\n",
    "val_iterator = iter(data.val_iter)\n",
    "print(\"Words: \", len(data.vocab.itos), \", On device: \", DEVICE.type)\n",
    "print(\"Loss Type: \", flags.losses)\n",
    "model = Model(data.vocab, data.tags, h_params, wvs=data.wvs)\n",
    "if DEVICE.type == 'cuda':\n",
    "    model.cuda(DEVICE)\n",
    "\n",
    "total_unsupervised_train_samples = len(data.train_iter)*BATCH_SIZE\n",
    "print(\"Unsupervised training examples: \", total_unsupervised_train_samples)\n",
    "current_time = time()\n",
    "#print(model)\n",
    "number_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.infer_bn.parameters() if p.requires_grad)\n",
    "print(\"Inference parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.gen_bn.parameters() if p.requires_grad)\n",
    "print(\"Generation parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.word_embeddings.parameters() if p.requires_grad)\n",
    "print(\"Embedding parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a man is playing with a ball .. ', ' a boy is riding a horse .. ', ' a man is sitting in a park .. ', ' a group of people are gathered by a building .. ', ' a group of people are walking in a competition .. ']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "def decode_to_text(x_hat_params, vocab_size, vocab_index):\n",
    "    # It is assumed that this function is used at test time for display purposes\n",
    "    # Getting the argmax from the one hot if it's not done\n",
    "    while x_hat_params.shape[-1] == vocab_size and x_hat_params.ndim > 3:\n",
    "        x_hat_params = x_hat_params.mean(0)\n",
    "    while x_hat_params.ndim > 2 and x_hat_params.shape[-1] != self.h_params.vocab_size:\n",
    "        x_hat_params = x_hat_params[0]\n",
    "    if x_hat_params.shape[-1] == vocab_size:\n",
    "        x_hat_params = torch.argmax(x_hat_params, dim=-1)\n",
    "    assert x_hat_params.ndim == 2, \"Mis-shaped generated sequence: {}\".format(x_hat_params.shape)\n",
    "    \n",
    "    samples = [' '.join([vocab_index.itos[w]\n",
    "                         for w in sen]).split('<eos>')[0].replace('<go>', '').replace('</go>', '')\n",
    "               .replace('<pad>', '_').replace('_unk', '<?>')\n",
    "               for sen in x_hat_params]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_sentences(mdl, n_samples, gen_len=16, sample_w=False, vary_z=True, complete=None):\n",
    "    \n",
    "            go_symbol = torch.ones([n_samples]).long() * \\\n",
    "                        mdl.index[mdl.generated_v].stoi['<go>']\n",
    "            go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "            x_prev = go_symbol\n",
    "            if complete is not None:\n",
    "                for token in complete.split(' '):\n",
    "                    x_prev = torch.cat([x_prev, torch.ones([n_samples, 1]).long().to(mdl.h_params.device) * \\\n",
    "                        mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "                gen_len = gen_len - len(complete.split(' '))\n",
    "            temp = 1.\n",
    "            z_gen = mdl.gen_bn.name_to_v['z1']\n",
    "            if vary_z:\n",
    "                z_sample = z_gen.prior_sample((n_samples,))[0]\n",
    "            else:\n",
    "                z_sample = z_gen.prior_sample((1,))[0]\n",
    "                z_sample = z_sample.repeat(n_samples, 1)\n",
    "            z_input = {'z1': z_sample.unsqueeze(1)}\n",
    "            # Structured Z case\n",
    "            z1, z2 = mdl.gen_bn.name_to_v['z2'], mdl.gen_bn.name_to_v['z3']\n",
    "            if vary_z:\n",
    "                mdl.gen_bn({'z1': z_sample.unsqueeze(1),\n",
    "                             'x_prev':torch.zeros((n_samples, 1, mdl.generated_v.size)).to(mdl.h_params.device)})\n",
    "                z1_sample, z2_sample = z1.post_samples.squeeze(1), z2.post_samples.squeeze(1)\n",
    "                z1_params, z2_params = z1.post_params, z2.post_params\n",
    "            else:\n",
    "                mdl.gen_bn({'z1': z_sample[0].unsqueeze(0).unsqueeze(1),\n",
    "                             'x_prev':torch.zeros((1, 1, mdl.generated_v.size)).to(mdl.h_params.device)})\n",
    "                z1_sample, z2_sample = z1.post_samples.squeeze(1).repeat(n_samples, 1), z2.post_samples.squeeze(1).repeat(n_samples, 1)\n",
    "                z1_params, z2_params = {k: v.squeeze(1).repeat(n_samples, 1) for k, v in z1.post_params.items()}, \\\n",
    "                                       {k: v.squeeze(1).repeat(n_samples, 1) for k, v in z2.post_params.items()}\n",
    "            z_input['z2'] = z1_sample.unsqueeze(1)\n",
    "            z_input['z3'] = z2_sample.unsqueeze(1)\n",
    "            \n",
    "            # Normal Autoregressive generation\n",
    "            for i in range(gen_len):\n",
    "                mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i+1, v.shape[-1])\n",
    "                                                  for k, v in z_input.items()}})\n",
    "                if not sample_w:\n",
    "                    samples_i = mdl.generated_v.post_params['logits']\n",
    "                else:\n",
    "                    samples_i = mdl.generated_v.posterior(logits=mdl.generated_v.post_params['logits']/temp,\n",
    "                                                           temperature=1).rsample()\n",
    "                x_prev = torch.cat([x_prev, torch.argmax(samples_i,     dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                                   dim=-1)\n",
    "\n",
    "            text = decode_to_text(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "            return text, {'z1':z_sample, 'z2':z1_sample, 'z3':z2_sample} , {'z2':z1_params, 'z3':z2_params}\n",
    "text, samples, params = get_sentences(model, 5, 16, sample_w=False, vary_z=True, complete=None)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a man and two men are playing a game .. ', ' a man is playing with a ball .. ', ' a little girl is getting ready to go to a train .. ', ' a person is playing with a ball .. ', ' a young girl is getting ready to go on a couch .. ', ' a young man is getting ready to go on a couch .. ', ' a man is playing with a ball .. ', ' there is a <?> outside .. ', ' a child is playing with a toy .. ', ' a man is playing with a ball .. ']\n[' a man is looking up .. ', ' a boy is riding a horse .. ', ' a man is looking up .. ', ' a man is riding a horse .. ', ' a small child is looking at a crowd .. ', ' a man is riding a horse .. ', ' a man is singing .. ', ' a little girl is looking at a boy .. ', ' a man is looking up .. ', ' a man is looking up .. ']\n[' a man is sitting in a park .. ', ' a man is sitting in a park .. ', ' a man is sitting in a park .. ', ' a band is playing in the sand .. ', ' a group of people are walking in front of a building .. ', ' a man is sitting in a park .. ', ' a man is sitting in a park .. ', ' people are standing in a store .. ', ' a group of people are walking in front of a building .. ', ' a man is sitting in a park .. ']\n[' a man is jumping over a piece of tree .. ', ' a person is jumping over a piece of tree .. ', ' a boy is jumping over a piece of tree .. ', ' a group of people are gathered by a building .. ', ' a man is jumping over a piece of tree .. ', ' a man is jumping over a piece of tree .. ', ' a man is jumping over a piece of tree .. ', ' a man is jumping over a piece of tree .. ', ' a man is jumping over a piece of tree .. ', ' a man is jumping over a piece of tree .. ']\n[' a man is in a red shirt .. ', ' a man is in a red shirt .. ', ' a man is in a red shirt .. ', ' a man is in a red shirt .. ', ' a boy is in a red shirt .. ', ' a man is in a red shirt .. ', ' a boy is in a red shirt .. ', ' a group of people are walking in a competition .. ', ' a man is in a red shirt .. ', ' a man is in a red shirt .. ']\n"
     ]
    }
   ],
   "source": [
    "def get_alternative_sentences(mdl, prev_latent_vals, params, var_z_ids, n_samples, gen_len, complete=None):\n",
    "            n_orig_sentences = prev_latent_vals['z1'].shape[0]\n",
    "            go_symbol = torch.ones([n_samples * n_orig_sentences]).long() * \\\n",
    "                        mdl.index[mdl.generated_v].stoi['<go>']\n",
    "            go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "            x_prev = go_symbol\n",
    "            if complete is not None:\n",
    "                for token in complete.split(' '):\n",
    "                    x_prev = torch.cat([x_prev, torch.ones([n_samples * n_orig_sentences, 1]).long().to(mdl.h_params.device) * \\\n",
    "                        mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "                gen_len = gen_len - len(complete.split(' '))\n",
    "            temp = 1.\n",
    "            orig_z = prev_latent_vals['z1'].repeat(n_samples, 1)\n",
    "            print(orig_z.shape)\n",
    "            orig_z1 = prev_latent_vals['z2'].repeat(n_samples, 1)\n",
    "            orig_z2 = prev_latent_vals['z3'].repeat(n_samples, 1)\n",
    "            z_gen, z1, z2 = mdl.gen_bn.name_to_v['z1'], mdl.gen_bn.name_to_v['z2'], mdl.gen_bn.name_to_v['z3']\n",
    "            \n",
    "            mdl.gen_bn({'z1': orig_z.unsqueeze(1), 'z2':orig_z1.unsqueeze(1),\n",
    "                        'z3': orig_z2.unsqueeze(1), \n",
    "                        'x_prev': torch.zeros((n_samples * n_orig_sentences, 1, mdl.generated_v.size)).to(mdl.h_params.device)})\n",
    "            z_sample = z_gen.prior_sample((n_samples * n_orig_sentences,))[0]\n",
    "            z1_sample, z2_sample = z1.post_samples.squeeze(1), z2.post_samples.squeeze(1)\n",
    "            z1_params, z2_params = z1.post_params, z2.post_params\n",
    "            for id in var_z_ids:\n",
    "                z_number = sum([id> sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "                z_index = id - sum(h_params.n_latents[:z_number])\n",
    "                start, end = int(h_params.z_size/max(h_params.n_latents)*z_index), int(h_params.z_size/max(h_params.n_latents)*(z_index+1))\n",
    "                source, destination = [z_sample, z1_sample, z2_sample][z_number], [orig_z, orig_z1, orig_z2][z_number]\n",
    "                destination[:, start:end] = source[:, start:end]\n",
    "            \n",
    "            z_input = {'z1': orig_z.unsqueeze(1), 'z2': orig_z1.unsqueeze(1), 'z3': orig_z2.unsqueeze(1)}\n",
    "            \n",
    "            # Normal Autoregressive generation\n",
    "            for i in range(gen_len):\n",
    "                mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i+1, v.shape[-1])\n",
    "                                                  for k, v in z_input.items()}})\n",
    "                samples_i = mdl.generated_v.post_params['logits']\n",
    "                \n",
    "                x_prev = torch.cat([x_prev, torch.argmax(samples_i,     dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                                   dim=-1)\n",
    "            \n",
    "            text = decode_to_text(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "            return text, {'z1': z_sample.tolist(), 'z2': z1_sample.tolist(), 'z3': z2_sample} , None#{'z1': z1_params, 'z2': z2_params}\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(48):\n",
    "#     alt_text, alt_samples, alt_params = get_alternative_sentences(model, {k:v[:5] for k, v in samples.items()},\n",
    "#                                                                   None,# [i for i in range(36,37)],\n",
    "#                                                                   [i],\n",
    "#                                                                   5, 16, complete=None)\n",
    "#     print(i, alt_text[2::5])\n",
    "\n",
    "alt_text, alt_samples, alt_params = get_alternative_sentences(model, {k:v[:5] for k, v in samples.items()},\n",
    "                                                              None,# [i for i in range(36,37)],\n",
    "                                                              [30],\n",
    "                                                              10, 16, complete=None)\n",
    "print(alt_text[0::5])\n",
    "print(alt_text[1::5])\n",
    "print(alt_text[2::5])\n",
    "print(alt_text[3::5])\n",
    "print(alt_text[4::5])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a man is playing with a ball .. ', ' a boy is riding a horse .. ', ' a man is sitting in a park .. ', ' a group of people are gathered by a building .. ', ' a group of people are walking in a competition .. ']\n[' a man is playing with a ball .. ', ' a group of people are outside .. ', ' a man is playing with a ball .. ', ' a group of people are outside .. ', ' a couple of people are outside .. ']\n[' a man is riding a horse .. ', ' a boy is riding a horse .. ', ' a man is riding a horse .. ', ' a group of people are outside .. ', ' a couple of people are outside in a restaurant .. ']\n[' a man is sitting in a park .. ', ' a group of people are walking in front of a building .. ', ' a man is sitting in a park .. ', ' a group of people are walking in front of a building .. ', ' a couple of people are walking in front of a building .. ']\n[' a man is jumping over a piece of tree .. ', ' a group of people are gathered by a building .. ', ' a man is jumping over a piece of tree .. ', ' a group of people are gathered by a building .. ', ' a couple of people are gathered by a building .. ']\n[' a man is in a red shirt .. ', ' a boy is in a red shirt .. ', ' a man is in a red shirt .. ', ' a group of people are walking .. ', ' a group of people are walking in a competition .. ']\n"
     ]
    }
   ],
   "source": [
    "def swap_latents(mdl, prev_latent_vals, var_z_ids, gen_len, complete=None):\n",
    "            n_orig_sentences = prev_latent_vals['z1'].shape[0]\n",
    "            n_samples = n_orig_sentences\n",
    "            go_symbol = torch.ones([n_samples * n_orig_sentences]).long() * \\\n",
    "                        mdl.index[mdl.generated_v].stoi['<go>']\n",
    "            go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "            x_prev = go_symbol\n",
    "            if complete is not None:\n",
    "                for token in complete.split(' '):\n",
    "                    x_prev = torch.cat([x_prev, torch.ones([n_samples * n_orig_sentences, 1]).long().to(mdl.h_params.device) * \\\n",
    "                        mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "                gen_len = gen_len - len(complete.split(' '))\n",
    "            temp = 1.\n",
    "            orig_z = prev_latent_vals['z1'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "            orig_z1 = prev_latent_vals['z2'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "            orig_z2 = prev_latent_vals['z3'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "            z_sample, z1_sample, z2_sample = orig_z.reshape(n_samples*n_orig_sentences, -1), orig_z1.reshape(n_samples*n_orig_sentences, -1), orig_z2.reshape(n_samples*n_orig_sentences, -1)\n",
    "            orig_z, orig_z1, orig_z2 = orig_z.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1), orig_z1.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1), \\\n",
    "                                             orig_z2.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "            \n",
    "\n",
    "            for id in var_z_ids:\n",
    "                z_number = sum([id> sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "                z_index = id - sum(h_params.n_latents[:z_number])\n",
    "                start, end = int(h_params.z_size/max(h_params.n_latents)*z_index), int(h_params.z_size/max(h_params.n_latents)*(z_index+1))\n",
    "                source, destination = [z_sample, z1_sample, z2_sample][z_number], [orig_z, orig_z1, orig_z2][z_number]\n",
    "                destination[:, start:end] = source[:, start:end]\n",
    "            \n",
    "            z_input = {'z1': orig_z.unsqueeze(1), 'z2': orig_z1.unsqueeze(1), 'z3': orig_z2.unsqueeze(1)}\n",
    "            \n",
    "            # Normal Autoregressive generation\n",
    "            for i in range(gen_len):\n",
    "                mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i+1, v.shape[-1])\n",
    "                                                  for k, v in z_input.items()}})\n",
    "                samples_i = mdl.generated_v.post_params['logits']\n",
    "                \n",
    "                x_prev = torch.cat([x_prev, torch.argmax(samples_i,     dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                                   dim=-1)\n",
    "            \n",
    "            text = decode_to_text(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "            return text, {'z1': z_sample.tolist(), 'z2': z1_sample.tolist(), 'z3': z2_sample} , None#{'z1': z1_params, 'z2': z2_params}\n",
    " \n",
    "alt_text, alt_samples, alt_params = swap_latents(model, {k:v[:5] for k, v in samples.items()},\n",
    "                                                              [30], 16, complete=None)\n",
    "\n",
    "print(text)\n",
    "print(alt_text[0::5])\n",
    "print(alt_text[1::5])\n",
    "print(alt_text[2::5])\n",
    "print(alt_text[3::5])\n",
    "print(alt_text[4::5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a man is holding a child ..   a toddler is taking a break from his mother .. \n(1, 1, 0, -1, [], [], [], ['', 'dobj'], [], ['sitting', 'wearing'])\n"
     ]
    }
   ],
   "source": [
    "from itertools import tee\n",
    "def get_depth(root, toks, tree, depth=0):\n",
    "    root_tree = list([tok for tok in tree[root]])\n",
    "    if len(root_tree)>0:\n",
    "        child_ids = [i for i, tok in enumerate(toks) if tok in root_tree]\n",
    "        return 1+max([get_depth(child_id, toks, tree) for child_id in child_ids])\n",
    "    else: return depth\n",
    "\n",
    "def get_sentence_statistics(orig, sen):\n",
    "    # Orig properties\n",
    "    orig_doc = nlp(orig)\n",
    "    orig_pos = [token.pos_ for token in orig_doc]\n",
    "    orig_toks = [token.text for token in orig_doc]\n",
    "    orig_length = len(orig_pos)\n",
    "    orig_dep_label = [token.dep_ for token in orig_doc]\n",
    "    orig_dep_tree = [list(token.children) for token in orig_doc]\n",
    "    orig_depth = get_depth(orig_dep_label.index('ROOT'), orig_doc, orig_dep_tree)\n",
    "    orig_root_children_text = [tok.text for tok in orig_dep_tree[orig_dep_label.index('ROOT')]]\n",
    "    orig_root_children_dep = [tok.dep_ for tok in orig_dep_tree[orig_dep_label.index('ROOT')]]\n",
    "    n_orig_root_children = len([tok.text for tok in orig_dep_tree[orig_dep_label.index('ROOT')]])\n",
    "    # Alt properties\n",
    "    doc = nlp(sen)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    toks = [token.text for token in doc]\n",
    "    length = len(pos)\n",
    "    dep_label = [token.dep_ for token in doc]\n",
    "    dep_tree = [list(token.children) for token in doc]\n",
    "    depth = get_depth(dep_label.index('ROOT'), doc, dep_tree)\n",
    "    root_children_text = [tok.text for tok in dep_tree[dep_label.index('ROOT')]]\n",
    "    root_children_dep = [tok.dep_ for tok in dep_tree[dep_label.index('ROOT')]]\n",
    "    n_root_children = len([tok.text for tok in dep_tree[dep_label.index('ROOT')]])\n",
    "    # Differences \n",
    "    len_diff = np.abs(length - orig_length)\n",
    "    depth_diff = np.abs(depth - orig_depth)\n",
    "    n_root_children_diff = np.abs(n_root_children-n_orig_root_children)\n",
    "    root_children_text_diff = np.union1d(np.setdiff1d(orig_root_children_text, root_children_text),\n",
    "                                        np.setdiff1d(root_children_text, orig_root_children_text)).tolist()\n",
    "    root_children_dep_diff = np.union1d(np.setdiff1d(orig_root_children_dep, root_children_dep),\n",
    "                                        np.setdiff1d(root_children_dep, orig_root_children_dep)).tolist()\n",
    "    new_deps = np.union1d(np.setdiff1d(orig_dep_label, dep_label), np.setdiff1d(dep_label, orig_dep_label)).tolist()\n",
    "    if len_diff:\n",
    "        word_diff = []\n",
    "        diff_pos = []\n",
    "        diff_dep = []\n",
    "        n_word_diff = -1\n",
    "    else:\n",
    "        word_diff = [(orig_tok, tok) for orig_tok, tok in zip(orig_toks, toks) if orig_tok != tok]\n",
    "        n_word_diff = len(word_diff)\n",
    "        diff_pos = [(orig_tok.pos_, tok.pos_) for orig_tok, tok in zip(orig_doc, doc) if orig_tok.text != tok.text]\n",
    "        diff_dep = [(orig_tok.dep_, tok.dep_) for orig_tok, tok in zip(orig_doc, doc) if orig_tok.text != tok.text]\n",
    "    return len_diff, depth_diff, n_root_children_diff, n_word_diff, word_diff, diff_pos, diff_dep, new_deps, root_children_dep_diff, \\\n",
    "           root_children_text_diff\n",
    "    \n",
    "print(text[0], alt_text[2])\n",
    "# print(get_sentence_statistics('a blond woman wearing a white shirt and white shirt .. ', 'a blond woman wearing a white shirt and white shirt .. '))\n",
    "print(get_sentence_statistics('a blond woman wearing a shirt .. ', 'a blond woman sitting  .. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:   0%|                                                                      | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:   2%|█▎                                                            | 1/48 [00:03<02:49,  3.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:   4%|██▌                                                           | 2/48 [00:07<02:46,  3.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:   6%|███▉                                                          | 3/48 [00:10<02:43,  3.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:   8%|█████▏                                                        | 4/48 [00:14<02:38,  3.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  10%|██████▍                                                       | 5/48 [00:17<02:33,  3.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  12%|███████▊                                                      | 6/48 [00:21<02:28,  3.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  15%|█████████                                                     | 7/48 [00:24<02:24,  3.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  17%|██████████▎                                                   | 8/48 [00:28<02:21,  3.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  19%|███████████▋                                                  | 9/48 [00:31<02:16,  3.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  21%|████████████▋                                                | 10/48 [00:35<02:10,  3.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  23%|█████████████▉                                               | 11/48 [00:38<02:06,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  25%|███████████████▎                                             | 12/48 [00:41<02:02,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  27%|████████████████▌                                            | 13/48 [00:45<01:58,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  29%|█████████████████▊                                           | 14/48 [00:48<01:54,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  31%|███████████████████                                          | 15/48 [00:51<01:50,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  33%|████████████████████▎                                        | 16/48 [00:55<01:47,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  35%|█████████████████████▌                                       | 17/48 [00:58<01:44,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  38%|██████████████████████▉                                      | 18/48 [01:02<01:41,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  40%|████████████████████████▏                                    | 19/48 [01:05<01:37,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  42%|█████████████████████████▍                                   | 20/48 [01:08<01:34,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  44%|██████████████████████████▋                                  | 21/48 [01:12<01:30,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  46%|███████████████████████████▉                                 | 22/48 [01:15<01:27,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  48%|█████████████████████████████▏                               | 23/48 [01:19<01:27,  3.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  50%|██████████████████████████████▌                              | 24/48 [01:22<01:23,  3.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  52%|███████████████████████████████▊                             | 25/48 [01:26<01:19,  3.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  54%|█████████████████████████████████                            | 26/48 [01:29<01:15,  3.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  56%|██████████████████████████████████▎                          | 27/48 [01:32<01:12,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  58%|███████████████████████████████████▌                         | 28/48 [01:36<01:07,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  60%|████████████████████████████████████▊                        | 29/48 [01:39<01:04,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  62%|██████████████████████████████████████▏                      | 30/48 [01:42<01:00,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  65%|███████████████████████████████████████▍                     | 31/48 [01:46<00:57,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  67%|████████████████████████████████████████▋                    | 32/48 [01:49<00:53,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  69%|█████████████████████████████████████████▉                   | 33/48 [01:53<00:50,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  71%|███████████████████████████████████████████▏                 | 34/48 [01:56<00:47,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  73%|████████████████████████████████████████████▍                | 35/48 [01:59<00:43,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  75%|█████████████████████████████████████████████▊               | 36/48 [02:03<00:40,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  77%|███████████████████████████████████████████████              | 37/48 [02:06<00:37,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  79%|████████████████████████████████████████████████▎            | 38/48 [02:09<00:33,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  81%|█████████████████████████████████████████████████▌           | 39/48 [02:13<00:30,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  83%|██████████████████████████████████████████████████▊          | 40/48 [02:16<00:27,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  85%|████████████████████████████████████████████████████         | 41/48 [02:19<00:23,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  88%|█████████████████████████████████████████████████████▍       | 42/48 [02:23<00:20,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  90%|██████████████████████████████████████████████████████▋      | 43/48 [02:26<00:17,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  92%|███████████████████████████████████████████████████████▉     | 44/48 [02:30<00:13,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  94%|█████████████████████████████████████████████████████████▏   | 45/48 [02:33<00:10,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  96%|██████████████████████████████████████████████████████████▍  | 46/48 [02:36<00:06,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  98%|███████████████████████████████████████████████████████████▋ | 47/48 [02:40<00:03,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:43<00:00,  3.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:43<00:00,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 1:   0%|                                                                      | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:   2%|█▎                                                            | 1/48 [00:03<02:35,  3.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:   4%|██▌                                                           | 2/48 [00:06<02:32,  3.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:   6%|███▉                                                          | 3/48 [00:10<02:30,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:   8%|█████▏                                                        | 4/48 [00:13<02:30,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  10%|██████▍                                                       | 5/48 [00:17<02:27,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  12%|███████▊                                                      | 6/48 [00:20<02:21,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  15%|█████████                                                     | 7/48 [00:23<02:18,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  17%|██████████▎                                                   | 8/48 [00:26<02:13,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  19%|███████████▋                                                  | 9/48 [00:30<02:10,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  21%|████████████▋                                                | 10/48 [00:33<02:06,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  23%|█████████████▉                                               | 11/48 [00:37<02:06,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  25%|███████████████▎                                             | 12/48 [00:40<02:01,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  27%|████████████████▌                                            | 13/48 [00:44<01:59,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  29%|█████████████████▊                                           | 14/48 [00:47<01:56,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  31%|███████████████████                                          | 15/48 [00:50<01:51,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  33%|████████████████████▎                                        | 16/48 [00:54<01:48,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  35%|█████████████████████▌                                       | 17/48 [00:57<01:44,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  38%|██████████████████████▉                                      | 18/48 [01:00<01:40,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  40%|████████████████████████▏                                    | 19/48 [01:04<01:37,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  42%|█████████████████████████▍                                   | 20/48 [01:07<01:33,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  44%|██████████████████████████▋                                  | 21/48 [01:10<01:30,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  46%|███████████████████████████▉                                 | 22/48 [01:14<01:26,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  48%|█████████████████████████████▏                               | 23/48 [01:17<01:23,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  50%|██████████████████████████████▌                              | 24/48 [01:20<01:19,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  52%|███████████████████████████████▊                             | 25/48 [01:24<01:18,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  54%|█████████████████████████████████                            | 26/48 [01:27<01:15,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  56%|██████████████████████████████████▎                          | 27/48 [01:31<01:11,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  58%|███████████████████████████████████▌                         | 28/48 [01:34<01:08,  3.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  60%|████████████████████████████████████▊                        | 29/48 [01:38<01:05,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  62%|██████████████████████████████████████▏                      | 30/48 [01:41<01:00,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  65%|███████████████████████████████████████▍                     | 31/48 [01:44<00:57,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  67%|████████████████████████████████████████▋                    | 32/48 [01:48<00:53,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  69%|█████████████████████████████████████████▉                   | 33/48 [01:51<00:51,  3.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  71%|███████████████████████████████████████████▏                 | 34/48 [01:55<00:47,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  73%|████████████████████████████████████████████▍                | 35/48 [01:58<00:44,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  75%|█████████████████████████████████████████████▊               | 36/48 [02:01<00:40,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  77%|███████████████████████████████████████████████              | 37/48 [02:05<00:37,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  79%|████████████████████████████████████████████████▎            | 38/48 [02:08<00:33,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  81%|█████████████████████████████████████████████████▌           | 39/48 [02:11<00:30,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  83%|██████████████████████████████████████████████████▊          | 40/48 [02:15<00:26,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  85%|████████████████████████████████████████████████████         | 41/48 [02:18<00:23,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  88%|█████████████████████████████████████████████████████▍       | 42/48 [02:21<00:19,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  90%|██████████████████████████████████████████████████████▋      | 43/48 [02:25<00:16,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  92%|███████████████████████████████████████████████████████▉     | 44/48 [02:28<00:13,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  94%|█████████████████████████████████████████████████████████▏   | 45/48 [02:32<00:10,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  96%|██████████████████████████████████████████████████████████▍  | 46/48 [02:35<00:06,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  98%|███████████████████████████████████████████████████████████▋ | 47/48 [02:38<00:03,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:42<00:00,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:42<00:00,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 2:   0%|                                                                      | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:   2%|█▎                                                            | 1/48 [00:03<02:35,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:   4%|██▌                                                           | 2/48 [00:06<02:33,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:   6%|███▉                                                          | 3/48 [00:10<02:32,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:   8%|█████▏                                                        | 4/48 [00:13<02:28,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  10%|██████▍                                                       | 5/48 [00:16<02:24,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  12%|███████▊                                                      | 6/48 [00:20<02:20,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  15%|█████████                                                     | 7/48 [00:23<02:16,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  17%|██████████▎                                                   | 8/48 [00:26<02:14,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  19%|███████████▋                                                  | 9/48 [00:30<02:09,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  21%|████████████▋                                                | 10/48 [00:33<02:06,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  23%|█████████████▉                                               | 11/48 [00:36<02:04,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  25%|███████████████▎                                             | 12/48 [00:40<02:00,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  27%|████████████████▌                                            | 13/48 [00:43<01:56,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  29%|█████████████████▊                                           | 14/48 [00:46<01:52,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  31%|███████████████████                                          | 15/48 [00:50<01:49,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  33%|████████████████████▎                                        | 16/48 [00:53<01:45,  3.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  35%|█████████████████████▌                                       | 17/48 [00:56<01:43,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  38%|██████████████████████▉                                      | 18/48 [01:00<01:40,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  40%|████████████████████████▏                                    | 19/48 [01:03<01:36,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  42%|█████████████████████████▍                                   | 20/48 [01:06<01:34,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  44%|██████████████████████████▋                                  | 21/48 [01:10<01:32,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  46%|███████████████████████████▉                                 | 22/48 [01:13<01:29,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  48%|█████████████████████████████▏                               | 23/48 [01:17<01:25,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  50%|██████████████████████████████▌                              | 24/48 [01:20<01:22,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  52%|███████████████████████████████▊                             | 25/48 [01:24<01:17,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  54%|█████████████████████████████████                            | 26/48 [01:27<01:14,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  56%|██████████████████████████████████▎                          | 27/48 [01:30<01:10,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  58%|███████████████████████████████████▌                         | 28/48 [01:34<01:07,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  60%|████████████████████████████████████▊                        | 29/48 [01:37<01:03,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  62%|██████████████████████████████████████▏                      | 30/48 [01:40<01:00,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  65%|███████████████████████████████████████▍                     | 31/48 [01:44<00:56,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  67%|████████████████████████████████████████▋                    | 32/48 [01:47<00:53,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  69%|█████████████████████████████████████████▉                   | 33/48 [01:51<00:51,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  71%|███████████████████████████████████████████▏                 | 34/48 [01:54<00:47,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  73%|████████████████████████████████████████████▍                | 35/48 [01:57<00:44,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  75%|█████████████████████████████████████████████▊               | 36/48 [02:01<00:40,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  77%|███████████████████████████████████████████████              | 37/48 [02:04<00:37,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  79%|████████████████████████████████████████████████▎            | 38/48 [02:07<00:33,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  81%|█████████████████████████████████████████████████▌           | 39/48 [02:11<00:30,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  83%|██████████████████████████████████████████████████▊          | 40/48 [02:14<00:27,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  85%|████████████████████████████████████████████████████         | 41/48 [02:18<00:23,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  88%|█████████████████████████████████████████████████████▍       | 42/48 [02:21<00:20,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  90%|██████████████████████████████████████████████████████▋      | 43/48 [02:24<00:16,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  92%|███████████████████████████████████████████████████████▉     | 44/48 [02:28<00:13,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  94%|█████████████████████████████████████████████████████████▏   | 45/48 [02:31<00:10,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  96%|██████████████████████████████████████████████████████████▍  | 46/48 [02:35<00:06,  3.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2:  98%|███████████████████████████████████████████████████████████▋ | 47/48 [02:38<00:03,  3.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:42<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 2: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:42<00:00,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 3:   0%|                                                                      | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:   2%|█▎                                                            | 1/48 [00:03<02:38,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:   4%|██▌                                                           | 2/48 [00:06<02:35,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:   6%|███▉                                                          | 3/48 [00:10<02:34,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:   8%|█████▏                                                        | 4/48 [00:13<02:30,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  10%|██████▍                                                       | 5/48 [00:16<02:25,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  12%|███████▊                                                      | 6/48 [00:20<02:22,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  15%|█████████                                                     | 7/48 [00:23<02:17,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  17%|██████████▎                                                   | 8/48 [00:26<02:13,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  19%|███████████▋                                                  | 9/48 [00:30<02:10,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  21%|████████████▋                                                | 10/48 [00:33<02:06,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  23%|█████████████▉                                               | 11/48 [00:36<02:02,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  25%|███████████████▎                                             | 12/48 [00:40<02:01,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  27%|████████████████▌                                            | 13/48 [00:43<01:58,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  29%|█████████████████▊                                           | 14/48 [00:47<01:57,  3.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  31%|███████████████████                                          | 15/48 [00:50<01:52,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  33%|████████████████████▎                                        | 16/48 [00:54<01:48,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  35%|█████████████████████▌                                       | 17/48 [00:57<01:44,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  38%|██████████████████████▉                                      | 18/48 [01:00<01:40,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  40%|████████████████████████▏                                    | 19/48 [01:04<01:37,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  42%|█████████████████████████▍                                   | 20/48 [01:07<01:34,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  44%|██████████████████████████▋                                  | 21/48 [01:10<01:30,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  46%|███████████████████████████▉                                 | 22/48 [01:14<01:26,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  48%|█████████████████████████████▏                               | 23/48 [01:17<01:23,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  50%|██████████████████████████████▌                              | 24/48 [01:20<01:19,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  52%|███████████████████████████████▊                             | 25/48 [01:24<01:16,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  54%|█████████████████████████████████                            | 26/48 [01:27<01:14,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  56%|██████████████████████████████████▎                          | 27/48 [01:30<01:10,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  58%|███████████████████████████████████▌                         | 28/48 [01:34<01:06,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  60%|████████████████████████████████████▊                        | 29/48 [01:37<01:03,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  62%|██████████████████████████████████████▏                      | 30/48 [01:41<01:01,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  65%|███████████████████████████████████████▍                     | 31/48 [01:44<00:57,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  67%|████████████████████████████████████████▋                    | 32/48 [01:47<00:54,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  69%|█████████████████████████████████████████▉                   | 33/48 [01:51<00:50,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  71%|███████████████████████████████████████████▏                 | 34/48 [01:54<00:46,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  73%|████████████████████████████████████████████▍                | 35/48 [01:57<00:43,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  75%|█████████████████████████████████████████████▊               | 36/48 [02:01<00:40,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  77%|███████████████████████████████████████████████              | 37/48 [02:04<00:37,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  79%|████████████████████████████████████████████████▎            | 38/48 [02:07<00:33,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  81%|█████████████████████████████████████████████████▌           | 39/48 [02:11<00:30,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  83%|██████████████████████████████████████████████████▊          | 40/48 [02:14<00:26,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  85%|████████████████████████████████████████████████████         | 41/48 [02:17<00:23,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  88%|█████████████████████████████████████████████████████▍       | 42/48 [02:21<00:19,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  90%|██████████████████████████████████████████████████████▋      | 43/48 [02:24<00:16,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  92%|███████████████████████████████████████████████████████▉     | 44/48 [02:27<00:13,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  94%|█████████████████████████████████████████████████████████▏   | 45/48 [02:31<00:09,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  96%|██████████████████████████████████████████████████████████▍  | 46/48 [02:34<00:06,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3:  98%|███████████████████████████████████████████████████████████▋ | 47/48 [02:37<00:03,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:41<00:00,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 3: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:41<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 4:   0%|                                                                      | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:   2%|█▎                                                            | 1/48 [00:03<02:39,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:   4%|██▌                                                           | 2/48 [00:06<02:35,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:   6%|███▉                                                          | 3/48 [00:10<02:31,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:   8%|█████▏                                                        | 4/48 [00:13<02:32,  3.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  10%|██████▍                                                       | 5/48 [00:17<02:29,  3.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  12%|███████▊                                                      | 6/48 [00:20<02:23,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  15%|█████████                                                     | 7/48 [00:23<02:19,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  17%|██████████▎                                                   | 8/48 [00:27<02:14,  3.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  19%|███████████▋                                                  | 9/48 [00:30<02:10,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  21%|████████████▋                                                | 10/48 [00:34<02:10,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  23%|█████████████▉                                               | 11/48 [00:37<02:06,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  25%|███████████████▎                                             | 12/48 [00:40<02:01,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  27%|████████████████▌                                            | 13/48 [00:44<01:57,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  29%|█████████████████▊                                           | 14/48 [00:47<01:55,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  31%|███████████████████                                          | 15/48 [00:51<01:53,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  33%|████████████████████▎                                        | 16/48 [00:54<01:48,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  35%|█████████████████████▌                                       | 17/48 [00:57<01:44,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  38%|██████████████████████▉                                      | 18/48 [01:01<01:40,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  40%|████████████████████████▏                                    | 19/48 [01:04<01:38,  3.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  42%|█████████████████████████▍                                   | 20/48 [01:07<01:34,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  44%|██████████████████████████▋                                  | 21/48 [01:11<01:30,  3.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  46%|███████████████████████████▉                                 | 22/48 [01:14<01:26,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  48%|█████████████████████████████▏                               | 23/48 [01:17<01:23,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  50%|██████████████████████████████▌                              | 24/48 [01:21<01:21,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  52%|███████████████████████████████▊                             | 25/48 [01:24<01:17,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  54%|█████████████████████████████████                            | 26/48 [01:27<01:13,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  56%|██████████████████████████████████▎                          | 27/48 [01:31<01:09,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  58%|███████████████████████████████████▌                         | 28/48 [01:34<01:06,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  60%|████████████████████████████████████▊                        | 29/48 [01:37<01:03,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  62%|██████████████████████████████████████▏                      | 30/48 [01:41<00:59,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  65%|███████████████████████████████████████▍                     | 31/48 [01:44<00:56,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  67%|████████████████████████████████████████▋                    | 32/48 [01:47<00:52,  3.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  69%|█████████████████████████████████████████▉                   | 33/48 [01:51<00:50,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  71%|███████████████████████████████████████████▏                 | 34/48 [01:54<00:46,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  73%|████████████████████████████████████████████▍                | 35/48 [01:57<00:43,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  75%|█████████████████████████████████████████████▊               | 36/48 [02:01<00:39,  3.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  77%|███████████████████████████████████████████████              | 37/48 [02:04<00:36,  3.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  79%|████████████████████████████████████████████████▎            | 38/48 [02:08<00:33,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  81%|█████████████████████████████████████████████████▌           | 39/48 [02:11<00:30,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  83%|██████████████████████████████████████████████████▊          | 40/48 [02:14<00:27,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  85%|████████████████████████████████████████████████████         | 41/48 [02:18<00:23,  3.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  88%|█████████████████████████████████████████████████████▍       | 42/48 [02:21<00:20,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  90%|██████████████████████████████████████████████████████▋      | 43/48 [02:25<00:16,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  92%|███████████████████████████████████████████████████████▉     | 44/48 [02:28<00:13,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  94%|█████████████████████████████████████████████████████████▏   | 45/48 [02:32<00:10,  3.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  96%|██████████████████████████████████████████████████████████▍  | 46/48 [02:35<00:06,  3.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4:  98%|███████████████████████████████████████████████████████████▋ | 47/48 [02:38<00:03,  3.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:42<00:00,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 4: 100%|█████████████████████████████████████████████████████████████| 48/48 [02:42<00:00,  3.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "header = ['original', 'altered', 'alteration_id', 'len_diff', 'depth_diff', 'n_root_children_diff', \n",
    "          'n_word_diff', 'word_diff', 'diff_pos', 'diff_dep', 'new_deps', 'root_children_dep_diff',\n",
    "           'root_children_text_diff']\n",
    "stats = []\n",
    "n_samples, n_alterations, nlatents = 100, 10, h_params.n_latents\n",
    "# Generating a hundred sentences\n",
    "text, samples, params = get_sentences(model, n_samples=n_samples, gen_len=16, sample_w=False, vary_z=True, complete=None)\n",
    "batch_size = 20\n",
    "for i in range(int(n_samples/batch_size)):\n",
    "    for j in tqdm(range(nlatents*3), desc=\"Processing sample {}\".format(str(i))):\n",
    "        # Altering the sentences\n",
    "        alt_text, _, _ = get_alternative_sentences(model, prev_latent_vals={k:v[i*batch_size:(i+1)*batch_size] for k, v in samples.items()},\n",
    "                                                   params=None, var_z_ids=[j], n_samples=n_alterations,\n",
    "                                                   gen_len=16, complete=None)\n",
    "        # Getting alteration statistics\n",
    "        for k in range(n_alterations*batch_size):\n",
    "            orig_text = text[(i*batch_size)+k%batch_size]\n",
    "            try:\n",
    "                len_diff, depth_diff, n_root_children_diff, n_word_diff, word_diff, diff_pos, diff_dep, new_deps, root_children_dep_diff, \\\n",
    "               root_children_text_diff = get_sentence_statistics(orig_text, alt_text[k])\n",
    "            except RecursionError:\n",
    "                print(orig_text, alt_text[k])\n",
    "                continue\n",
    "            stats.append([orig_text, alt_text[k], j, len_diff, depth_diff, n_root_children_diff, n_word_diff, \n",
    "                          word_diff, diff_pos, diff_dep, new_deps, root_children_dep_diff, \n",
    "                          root_children_text_diff])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]\n               len_diff  depth_diff  n_root_children_diff\nalteration_id                                            \n18             0.012422    0.011952              0.033333\n20             0.013199    0.035857              0.040741\n31             0.010093    0.035857              0.033333\n25             0.028727    0.035857              0.018519\n26             0.020963    0.035857              0.014815\n41             0.012422    0.039841              0.014815\n21             0.014752    0.043825              0.022222\n27             0.054348    0.047809              0.059259\n19             0.058230    0.051793              0.048148\n17             0.060559    0.051793              0.070370\n32             0.016304    0.051793              0.029630\n24             0.034161    0.055777              0.066667\n28             0.027950    0.055777              0.018519\n12             0.106366    0.059761              0.159259\n34             0.034161    0.059761              0.018519\n23             0.041925    0.059761              0.022222\n45             0.048137    0.067729              0.107407\n36             0.027174    0.071713              0.022222\n42             0.106366    0.075697              0.081481\n29             0.054348    0.079681              0.111111\n6              0.097050    0.083665              0.137037\n22             0.014752    0.083665              0.051852\n16             0.009317    0.087649              0.044444\n13             0.100932    0.103586              0.122222\n47             0.116460    0.107570              0.144444\n15             0.107143    0.107570              0.129630\n40             0.068323    0.115538              0.100000\n4              0.113354    0.119522              0.077778\n2              0.123447    0.123506              0.140741\n46             0.178571    0.131474              0.200000\n5              0.147516    0.143426              0.188889\n0              0.099379    0.155378              0.170370\n44             0.052019    0.155378              0.055556\n1              0.135870    0.159363              0.148148\n7              0.142081    0.167331              0.129630\n38             0.124224    0.171315              0.103704\n14             0.128106    0.187251              0.222222\n9              0.134317    0.191235              0.177778\n11             0.130435    0.191235              0.125926\n37             0.227484    0.239044              0.262963\n8              0.142857    0.266932              0.192593\n3              0.279503    0.334661              0.274074\n39             0.255435    0.354582              0.259259\n33             0.392857    0.430279              0.462963\n43             0.344720    0.442231              0.303704\n35             0.324534    0.466135              0.359259\n10             0.839286    0.988048              1.000000\n30             1.000000    1.000000              0.807407\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "header = ['original', 'altered', 'alteration_id', 'len_diff', 'depth_diff', 'n_root_children_diff', \n",
    "          'n_word_diff', 'word_diff', 'diff_pos', 'diff_dep', 'new_deps', 'root_children_dep_diff',\n",
    "           'root_children_text_diff']\n",
    "#df = pd.DataFrame(stats, columns=header)\n",
    "df = pd.read_csv('Autoreg5stats.csv')\n",
    "print(df['alteration_id'].unique())\n",
    "grouped = df.groupby('alteration_id')\n",
    "diff_df = grouped.mean()[['len_diff', 'depth_diff', 'n_root_children_diff']]\n",
    "diff_df['len_diff'] /= diff_df['len_diff'].max()\n",
    "diff_df['depth_diff'] /= diff_df['depth_diff'].max()\n",
    "diff_df['n_root_children_diff'] /= diff_df['n_root_children_diff'].max()\n",
    "print(diff_df.sort_values('depth_diff', axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('amod', 'compound'), ('pobj', 'pobj')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-24f32f823f49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# df['new_deps'] = df['new_deps'].map(revert_to_l1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# df['root_children_dep_diff'] = df['root_children_dep_diff'].map(revert_to_l1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0md_dep_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'd_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mty\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mty\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'diff_dep'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mty\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_dep_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mconcerned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# print(np.unique(df['new_deps'].array, return_counts=True))#'root_children_diff',\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "header = ['original', 'altered', 'alteration_id', 'len_diff', 'depth_diff', 'n_root_children_diff', \n",
    "          'n_word_diff', 'word_diff', 'diff_pos', 'diff_dep', 'new_deps', 'root_children_dep_diff',\n",
    "           'root_children_text_diff']\n",
    "print(df['diff_dep'].array[116], )\n",
    "def revert_to_l1(el):\n",
    "    if len(el[1:-1]):\n",
    "        el = el.replace('(', '').replace(\"'\", '').replace(' ', '').replace('),', ')').replace(']', '').replace('[', '')\n",
    "        output = [el_i.split(\",\") for el_i in el.split(')') if len(el_i)>3]\n",
    "        if len(output)>1: \n",
    "            output = np.concatenate(output)\n",
    "        return np.unique(output)\n",
    "    else:\n",
    "        return []\n",
    "# df['diff_dep'] = df['diff_dep'].map(revert_to_l1)\n",
    "# df['new_deps'] = df['new_deps'].map(revert_to_l1)\n",
    "# df['root_children_dep_diff'] = df['root_children_dep_diff'].map(revert_to_l1)\n",
    "d_dep_types = ['d_'+ty for ty in np.unique(np.concatenate(df['diff_dep'].array))]\n",
    "for ty in tqdm(d_dep_types):\n",
    "    concerned = []\n",
    "    for deps in df['diff_dep'].array.astype(list):\n",
    "        concerned.append(ty[2:] in deps)\n",
    "    df[ty] = concerned\n",
    "\n",
    "dep_types = np.unique(np.concatenate(df['new_deps'].array))\n",
    "for ty in tqdm(dep_types):\n",
    "    concerned = []\n",
    "    for deps in df['new_deps'].array:\n",
    "        concerned.append(ty in deps)\n",
    "    df[ty] = concerned\n",
    "c_dep_types = ['c_'+ty for ty in np.unique(np.concatenate(df['root_children_dep_diff'].array))]\n",
    "for ty in tqdm(c_dep_types):\n",
    "    concerned = []\n",
    "    for deps in df['root_children_dep_diff'].array:\n",
    "        concerned.append(ty[2:] in deps)\n",
    "    df[ty] = concerned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ ANY ****************\n       *** Values ***\nacl          0.010\nacomp        0.078\nadvcl        0.013\nadvmod       0.068\nagent        0.014\namod         0.326\nattr         0.030\naux          0.111\nauxpass      0.017\ncc           0.029\ncompound     0.024\nconj         0.029\ndep          0.013\ndet          0.004\ndobj         0.369\nexpl         0.020\nmark         0.005\nneg          0.005\nnpadvmod     0.001\nnsubj        0.030\nnsubjpass    0.017\nnummod       0.047\npobj         0.343\nposs         0.073\nprep         0.348\nprt          0.054\nrelcl        0.002\nxcomp        0.090\ndtype: float64\n       ***  IDX   ***\nacl          30\nacomp        10\nadvcl        43\nadvmod       10\nagent        10\namod         30\nattr         30\naux          30\nauxpass      10\ncc           30\ncompound     43\nconj         30\ndep          10\ndet          10\ndobj         10\nexpl         30\nmark         43\nneg          33\nnpadvmod     10\nnsubj        10\nnsubjpass    10\nnummod       30\npobj         10\nposs         10\nprep         10\nprt          10\nrelcl        30\nxcomp        10\ndtype: int64\n************ ROOT ****************\n       *** Values ***\nc_acl          0.002\nc_acomp        0.078\nc_advcl        0.005\nc_advmod       0.060\nc_agent        0.014\nc_amod         0.003\nc_attr         0.030\nc_aux          0.138\nc_auxpass      0.017\nc_cc           0.003\nc_conj         0.003\nc_dep          0.011\nc_det          0.004\nc_dobj         0.356\nc_expl         0.020\nc_neg          0.004\nc_npadvmod     0.001\nc_nsubj        0.030\nc_nsubjpass    0.017\nc_nummod       0.001\nc_prep         0.384\nc_prt          0.054\nc_punct        0.001\nc_xcomp        0.021\ndtype: float64\n       ***  IDX   ***\nc_acl          10\nc_acomp        10\nc_advcl        43\nc_advmod       30\nc_agent        10\nc_amod          3\nc_attr         30\nc_aux          10\nc_auxpass      10\nc_cc           35\nc_conj         35\nc_dep          10\nc_det          30\nc_dobj         10\nc_expl         30\nc_neg          33\nc_npadvmod     39\nc_nsubj        10\nc_nsubjpass    10\nc_nummod       10\nc_prep         10\nc_prt          10\nc_punct        42\nc_xcomp        10\ndtype: int64\n************ Same length differences ****************\n       *** Values ***\nd_ROOT         0.207\nd_acl          0.003\nd_acomp        0.011\nd_advcl        0.002\nd_advmod       0.023\nd_amod         0.042\nd_attr         0.010\nd_aux          0.062\nd_cc           0.004\nd_compound     0.008\nd_conj         0.010\nd_dep          0.001\nd_det          0.082\nd_dobj         0.103\nd_expl         0.010\nd_neg          0.002\nd_nsubj        0.203\nd_nsubjpass    0.008\nd_nummod       0.016\nd_pobj         0.240\nd_poss         0.024\nd_prep         0.162\nd_prt          0.004\nd_punct        0.008\nd_xcomp        0.013\ndtype: float64\n       ***  IDX   ***\nd_ROOT         10\nd_acl          30\nd_acomp        33\nd_advcl        33\nd_advmod       30\nd_amod         10\nd_attr         10\nd_aux          30\nd_cc           10\nd_compound     11\nd_conj          0\nd_dep          33\nd_det          10\nd_dobj         35\nd_expl         45\nd_neg          30\nd_nsubj        30\nd_nsubjpass    30\nd_nummod       30\nd_pobj         43\nd_poss         13\nd_prep         10\nd_prt          47\nd_punct        10\nd_xcomp        10\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "header = ['original', 'altered', 'alteration_id', 'len_diff', 'depth_diff', 'n_root_children_diff', \n",
    "          'n_word_diff', 'word_diff', 'diff_pos', 'diff_dep', 'new_deps', 'root_children_dep_diff',\n",
    "           'root_children_text_diff']\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option('display.width', 150)\n",
    "pd.options.display.max_rows = 10000\n",
    "\n",
    "\n",
    "grouped = df.groupby('alteration_id')\n",
    "# print(grouped.mean()[dep_types])\n",
    "print('************ ANY ****************')\n",
    "print('       *** Values ***')\n",
    "print(grouped.mean()[dep_types].max())\n",
    "print('       ***  IDX   ***')\n",
    "print(grouped.mean()[dep_types].idxmax())\n",
    "print('************ ROOT ****************')\n",
    "print('       *** Values ***')\n",
    "print(grouped.mean()[c_dep_types].max())\n",
    "print('       ***  IDX   ***')\n",
    "print(grouped.mean()[c_dep_types].idxmax())\n",
    "print('************ Same length differences ****************')\n",
    "print('       *** Values ***')\n",
    "print(grouped.mean()[d_dep_types].max())\n",
    "print('       ***  IDX   ***')\n",
    "print(grouped.mean()[d_dep_types].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a man is standing in front of a street ..   a man with a black shirt is walking down a street .. \n a young boy is walking with a red toy ..   a young boy is running outside .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a man with a pink hat is swimming ..   a man with a blue hat is swimming .. \n a dog is sitting in a field with a blue shirt ..   a dog is sitting in a race with a blue shirt .. \n a man in a green shirt is playing a rock outside ..   a man is laying on a couch .. \n a boy is sitting on a couch ..   a boy is sitting in a chair .. \n a little boy is preparing to play with a ball ..   a little boy is jumping off a mountain with a red shirt .. \n a little boy is swimming in a pond ..   a little boy is jumping in a pond .. \n a young boy is walking with a red toy ..   a young boy is sitting outside .. \n a man is sitting in a park ..   a man is sitting outside .. \n a man with a pink hat is swimming ..   a man with a hat is looking at a statue .. \n a man in a green shirt is playing a rock outside ..   a man is laying down .. \n a little boy is preparing to play with a ball ..   a little boy is swimming up in the water .. \n a person is walking down the street ..   a person is outside .. \n a little boy is swimming in a pond ..   a little boy is jumping in a pond .. \n a man is standing in front of a street ..   a man with a jacket is sitting on his cellphone .. \n a young boy is walking with a red toy ..   a young boy is in a blue shirt .. \n a man with a pink hat is swimming ..   a man with a hat is looking at a statue .. \n a little boy is preparing to play with a ball ..   a little boy is chasing a <?> .. \n a person is walking down the street ..   a person is walking in the street .. \n a little boy is swimming in a pond ..   a little boy is running in the snow .. \n a man is standing in front of a street ..   a man with a long hair is walking down the street .. \n a young boy is walking with a red toy ..   a young boy is jumping with a red toy .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a man with a pink hat is swimming ..   a man with a black hat is swimming .. \n a dog is sitting in a field with a blue shirt ..   a dog is sitting in a race with a blue shirt .. \n a man in a green shirt is playing a rock outside ..   a man is laying on a couch .. \n a little boy is preparing to play with a ball ..   a little boy is jumping off a mountain with a red shirt .. \n a person is walking down the street ..   a person is outside on a bench .. \n a little boy is swimming in a pond ..   a little boy is wearing a red shirt .. \n a man is standing in front of a street ..   a man with a long hair is walking down the street .. \n a young boy is walking with a red toy ..   a young boy is outside with a red toy .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a man with a pink hat is swimming ..   a man with a hat is looking at a statue .. \n a dog is sitting in a field with a blue shirt ..   a dog is sitting in a race with a blue shirt .. \n a man in a green shirt is playing a rock outside ..   a man in a pink shirt is walking .. \n a little boy is preparing to play with a ball ..   a little boy is jumping off a mountain with a red shirt .. \n a person is walking down the street ..   a person is outside .. \n a little boy is swimming in a pond ..   a little boy is jumping in a pond .. \n a man is standing in front of a street ..   a man with a black shirt is walking down a street .. \n a young boy is walking with a red toy ..   a young boy is jumping with a red toy .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a man with a pink hat is swimming ..   a man with a black hat is swimming .. \n a man in a green shirt is playing a rock outside ..   a man is laying down .. \n a little boy is preparing to play with a ball ..   a little boy is swimming up in the water .. \n a person is walking down the street ..   a person is walking on the street .. \n a little boy is swimming in a pond ..   a little boy is jumping in a pond .. \n a man is standing in front of a street ..   a man with a black shirt is walking down a street .. \n a young boy is walking with a red toy ..   a young boy is jumping with a red toy .. \n a man with a pink hat is swimming ..   a man with a blue hat is swimming .. \n a dog is sitting in a field with a blue shirt ..   a dog is sitting at a table with a blue shirt .. \n a man in a green shirt is playing a rock outside ..   a man is laying on a couch .. \n a little boy is preparing to play with a ball ..   a little boy is trying to get his <?> .. \n a little boy is swimming in a pond ..   a little boy is riding a bike .. \n a man is looking at a lot of people ..   a man is looking for a lot of people .. \n a young boy is walking with a red toy ..   a young boy is playing with a red toy .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a man with a pink hat is swimming ..   a man with a hat is looking at a statue .. \n a man in a green shirt is playing a rock outside ..   a man is laying on a couch .. \n a person is walking down the street ..   a person is outside in a parade .. \n a little boy is swimming in a pond ..   a little boy is jumping in a pond .. \n a young boy is walking with a red toy ..   a young boy is taking a nap .. \n a man is sitting in a park ..   a man is sitting at a bar .. \n a man in a green shirt is playing a rock outside ..   a man is laying on a couch .. \n a little boy is preparing to play with a ball ..   a little boy is swimming up in the water .. \n a person is walking down the street ..   a person is outside in a parade .. \n a little boy is swimming in a pond ..   a little boy is riding a bike .. \n a man is standing in front of a street ..   a man with a long hair is walking down the street .. \n a young boy is walking with a red toy ..   a young boy is riding a bicycle outside .. \n a man with a pink hat is swimming ..   a man with a hat is looking at a statue .. \n a group of people are sitting on a couch ..   a man is dancing .. \n a dog is sitting in a field with a blue shirt ..   a dog is sitting on a bench with a blue shirt .. \n a man in a green shirt is playing a rock outside ..   a man is laying on a couch .. \n a person is walking down the street ..   a person is outside in a parade .. \n a little boy is swimming in a pond ..   a little boy is running in the snow .. \n a man is looking at a lot of people ..   a man is looking up .. \n a man is standing near a train ..   a man is standing on a couch .. \n there is a man on the street ..   there is a man sitting on the beach .. \n a boy is sitting in a park ..   a boy is sitting inside .. \n a small child is laying on a bench ..   a small child is walking on a track .. \n a man is riding a bike ..   a man is riding a horse .. \n a young girl is jumping up ..   a young girl is playing on a sidewalk .. \n a young man is wearing a red shirt ..   a young man is looking for a lot of paper .. \n a man is standing near a train ..   a man is standing on a couch .. \n there is a man on the street ..   there is a man walking down the road .. \n a man is playing a game ..   a man is playing with a ball .. \n a young girl is playing outside ..   a young girl is walking outside .. \n a boy is sitting in a park ..   a boy is sitting at a table .. \n a man is riding a bike ..   a man is riding a horse .. \n a young girl is jumping up ..   a young girl is playing on a sidewalk .. \n a little boy is sitting on a bench ..   a little boy is laying on a bench .. \n a young man is wearing a red shirt ..   a young man is looking for a lot of paper .. \n a man is standing in front of a tree ..   a man is playing with a ball .. \n there is a man on the street ..   there is a man sitting on the beach .. \n a man with a blue shirt is about to school ..   a man with a red shirt is about to school .. \n a young girl is playing outside ..   a young girl is sitting outside .. \n a boy is sitting in a park ..   a boy is sitting on a bench .. \n a small child is laying on a bench ..   a small child is walking on a track .. \n a man is riding a bike ..   a man is riding a horse .. \n a young girl is jumping up ..   a young girl is running through the snow .. \n a young man is wearing a red shirt ..   a young man is running on a bench .. \n a man is standing in front of a tree ..   a man is playing with a ball .. \n a man is standing near a train ..   a man is standing outside .. \n a man is <?> his <?> ..   a man is <?> .. \n there is a man on the street ..   there is a man who is having a cigarette .. \n a man is playing a game ..   a man is playing with a ball .. \n a man is riding a bike ..   a man is riding a horse .. \n a young girl is jumping up ..   a young girl is doing a trick .. \n a little boy is sitting on a bench ..   a little boy is walking down the street .. \n a man is standing near a train ..   a man is standing outside .. \n there is a man on the street ..   there is a man in a yellow shirt .. \n a boy is sitting in a park ..   a boy is sitting at a table .. \n a man is riding a bike ..   a man is riding a horse .. \n a young girl is jumping up ..   a young girl is in a dress .. \n a young man is wearing a red shirt ..   a young man is looking for a picture .. \n a man is standing near a train ..   a man is standing on a couch .. \n a man is <?> his <?> ..   a man is <?> .. \n there is a man on the street ..   there is a man in a yellow shirt .. \n a man with a blue shirt is about to school ..   a man with a long black and yellow phone .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a man is riding a bike ..   a man is riding a horse .. \n a young man is wearing a red shirt ..   a young man is running on a bench .. \n a man is standing near a train ..   a man is standing on a couch .. \n a man is <?> his <?> ..   a man is <?> .. \n there is a man on the street ..   there is a man in a yellow shirt .. \n a young girl is playing outside ..   a young girl is sitting outside .. \n a boy is sitting in a park ..   a boy is sitting at a table .. \n a young girl is jumping up ..   a young girl is playing in a tree .. \n a young man is wearing a red shirt ..   a young man is riding a bike .. \n a man is standing in front of a tree ..   a man is standing next to a wall .. \n a man is standing near a train ..   a man is standing on a couch .. \n a man is <?> his <?> ..   a man is <?> .. \n there is a man on the street ..   there is a man in a yellow shirt .. \n a boy is sitting in a park ..   a boy is sitting at a table .. \n a small child is laying on a bench ..   a small child is holding a baby .. \n a man is riding a bike ..   a man is riding a horse .. \n a young girl is jumping up ..   a young girl is in a dress .. \n a young man is wearing a red shirt ..   a young man is riding a bike .. \n a man is standing in front of a tree ..   a man is playing with a ball .. \n a man is standing near a train ..   a man is standing outside .. \n a man is <?> his <?> ..   a man is <?> .. \n there is a man on the street ..   there is a man sitting on the beach .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a small child is laying on a bench ..   a small child is sitting on a bench .. \n a young girl is jumping up ..   a young girl is smiling for her camera .. \n a young man is wearing a red shirt ..   a young man is riding a bike .. \n a man is standing near a train ..   a man is standing by a man .. \n a man is <?> his <?> ..   a man is <?> .. \n there is a man on the street ..   there is a man sitting on the beach .. \n a young girl is playing outside ..   a young girl is walking outside .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a boy is sitting in a park ..   a boy is sitting on a bench .. \n a man is riding a bike ..   a man is riding a horse .. \n a young girl is jumping up ..   a young girl is walking down the sidewalk .. \n a young man is wearing a red shirt ..   a young man is running on a bench .. \n a person is jumping through the grass ..   a person is jumping in a lake .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a small boy is sitting on a bench ..   a small boy is walking on a bench .. \n two young girls are playing in a field ..   two young girls are walking in the street .. \n a man is standing on a bench ..   a man is holding a dog .. \n a person is jumping through the grass ..   a person is jumping into a lake .. \n a man is sitting in a chair ..   a man is holding a bicycle .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a group of people are at a table ..   a group of people are sitting down on a street .. \n a man is standing on a bench ..   a man is standing in a field .. \n a person is jumping through the grass ..   a person is jumping in a lake .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a small boy is sitting on a bench ..   a small boy is at a table .. \n a child is being held by a building ..   a child is being photographed by a building .. \n a man with a hat is looking at a camera ..   a man with a hair is looking at a woman .. \n a person is jumping through the grass ..   a person is jumping into a lake .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a child is being held by a building ..   a child is being photographed by a building .. \n a man with a hat is looking at a camera ..   a man with a black shirt is sitting on a beach .. \n a man is standing on a bench ..   a man is standing by a wall .. \n a person is jumping through the grass ..   a person is jumping in a lake .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a man is standing on a phone ..   a man is standing around .. \n a man with a hat is looking at a camera ..   a man with a black shirt is sitting on a beach .. \n a person is jumping through the grass ..   a person is jumping in a lake .. \n a man is sitting in a chair ..   a man is holding a bicycle .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a man is in a red shirt ..   a man is in a pink shirt .. \n a person is jumping through the grass ..   a person is jumping in a lake .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a child is being held by a building ..   a child is being photographed by a building .. \n a man with a hat is looking at a camera ..   a man with a black shirt is sitting on a beach .. \n a person is jumping through the grass ..   a person is jumping in a lake .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a man is standing on a phone ..   a man is standing around .. \n a small boy is sitting on a bench ..   a small boy is walking on a bench .. \n a group of people are at a table ..   a group of people are sitting down on a street .. \n a man is going to a baseball game ..   a man is going for a baseball game .. \n a person is jumping through the grass ..   a person is jumping in a lake .. \n a man is sitting in a chair ..   a man is holding a bicycle .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a group of people are at a table ..   a group of people are sitting down on a street .. \n a man with a hat is looking at a camera ..   a man with a pink shirt is on a beach .. \n a person is jumping through the grass ..   a person is jumping on a trail .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a man is standing on a phone ..   a man is standing in front of a truck .. \n a group of people are at a table ..   a group of people are sitting down on a street .. \n a man with a hat is looking at a camera ..   a man with a black shirt is sitting on a beach .. \n a man is standing on a sidewalk ..   a man is standing near a red mountain .. \n a man is wearing a red shirt and a boy ..   a man is wearing a red shirt and a child .. \n a small girl is playing in the street ..   a small girl is performing .. \n a girl is working in a <?> ..   a girl is working on a bike .. \n a guy is jumping in a field ..   a guy is jumping on a bench .. \n a man is standing in a park ..   a man is standing on a bench .. \n a young boy is playing in a field ..   a young boy is jumping on a beach .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a boy is making a skateboard ..   a boy is riding a bicycle .. \n a young girl is holding a lot of guitar ..   a young girl is in a costume .. \n a guy is jumping in a field ..   a guy is jumping on a bench .. \n a young boy is playing in a field ..   a young boy is sleeping on a beach .. \n a man is sitting in a chair ..   a man is sitting at a table .. \n a boy is making a skateboard ..   a boy is riding a bicycle .. \n a young girl is holding a lot of guitar ..   a young girl is playing with her daughter .. \n a man is standing on a sidewalk ..   a man is standing outside .. \n a small girl is playing in the street ..   a small girl is walking on the street .. \n a young boy is playing in a field ..   a young boy is jumping on a beach .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a man is standing on a sidewalk ..   a man is standing outside .. \n a man is standing in front of a lake ..   a man is standing on a bench .. \n a girl is working in a <?> ..   a girl is working on a bike .. \n a guy is jumping in a field ..   a guy is jumping on a bench .. \n a young boy is playing in a field ..   a young boy is in a room .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a boy is making a skateboard ..   a boy is riding a bicycle .. \n a young girl is holding a lot of guitar ..   a young girl is playing with her daughter .. \n a man is playing with a child ..   a man is playing a guitar .. \n a man is wearing a red shirt and a boy ..   a man is wearing a red shirt and a child .. \n a man is standing in a park ..   a man is standing on a bench .. \n a young boy is playing in a field ..   a young boy is in a room .. \n a man is sitting in a chair ..   a man is sitting down with his friend .. \n a boy is making a skateboard ..   a boy is riding a horse .. \n a small girl is playing in the street ..   a small girl is walking on the street .. \n a girl is working in a <?> ..   a girl is working on a bike .. \n a man is standing in a park ..   a man is standing on a bench .. \n a young boy is playing in a field ..   a young boy is in a room .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a boy is making a skateboard ..   a boy is riding a bicycle .. \n a man is standing in front of a lake ..   a man is standing on a bench .. \n a small girl is playing in the street ..   a small girl is walking on the street .. \n a guy is jumping in a field ..   a guy is jumping into a lake .. \n a young boy is playing in a field ..   a young boy is sleeping on a beach .. \n a man is sitting in a chair ..   a man is inside a building .. \n a boy is making a skateboard ..   a boy is riding a bicycle .. \n a young girl is holding a lot of guitar ..   a young girl is playing with her daughter .. \n a man is standing on a sidewalk ..   a man is standing near a city street .. \n a young boy is playing in a field ..   a young boy is in a room .. \n a young girl is holding a lot of guitar ..   a young girl is playing with her daughter .. \n a man is playing with a child ..   a man is playing a guitar .. \n a small girl is playing in the street ..   a small girl is walking on the street .. \n a man is standing in a park ..   a man is standing by a building .. \n a young boy is playing in a field ..   a young boy is in a room .. \n a man is sitting in a chair ..   a man is sitting at a table .. \n a boy is making a skateboard ..   a boy is riding a horse .. \n a young girl is holding a lot of guitar ..   a young girl is playing with her daughter .. \n a man is standing on a sidewalk ..   a man is standing outside .. \n a man is standing in front of a lake ..   a man is standing on a bench .. \n a girl is working in a <?> ..   a girl is working on a bike .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a young boy is playing in a field ..   a young boy is in a room .. \n a man is sitting in a chair ..   a man is sitting on a bench .. \n a boy is making a skateboard ..   a boy is riding a horse .. \n a young girl is holding a lot of guitar ..   a young girl is playing with her daughter .. \n a man is sitting down ..   a man is sitting in a chair .. \n a boy is jumping on a motorcycle in his mouth ..   a boy is jumping into a lake in his mouth .. \n a man is standing in a field ..   a man is standing on a beach .. \n a man is outside in a park ..   a man is outside .. \n a man is sitting in a park ..   a man is sitting down .. \n a man in a white shirt is standing in a restaurant ..   a man in a white shirt is running in a field .. \n a young boy is playing with a cat ..   a young boy is sitting on a back .. \n a small boy is running in the snow ..   a small boy is sitting on a bench .. \n a man is sitting on his phone ..   a man is sitting outside .. \n a man is sitting down ..   a man is sitting in a chair .. \n a man is standing in a field ..   a man is standing outside .. \n a man is outside in a park ..   a man is outside playing with a ball .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a white shirt is running in a field .. \n a young boy is playing with a cat ..   a young boy is in a chair .. \n a man is sitting down ..   a man is sitting on a bench .. \n a man is standing in a field ..   a man is standing outside .. \n a man is outside in a park ..   a man is outside .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a dress is looking at a camera .. \n a small boy is running in the snow ..   a small boy is looking at a horse .. \n a man is sitting down ..   a man is sitting in a chair .. \n a man is standing in a field ..   a man is standing on a beach .. \n a person is getting ready to take a picture ..   a person is getting ready for a how ball .. \n a man is outside in a park ..   a man is outside .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a pink shirt is running in his mouth .. \n a small boy is running in the snow ..   a small boy is looking at a horse .. \n a man is sitting on his phone ..   a man is sitting in a park .. \n a man is sitting down ..   a man is sitting in a chair .. \n a man is standing in a field ..   a man is standing on a beach .. \n a man is outside in a park ..   a man is outside playing with a ball .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a green shirt is running in his mouth .. \n a young boy is playing with a cat ..   a young boy is sitting on a back .. \n a small boy is running in the snow ..   a small boy is looking at a horse .. \n a man is sitting on his phone ..   a man is sitting in a park .. \n a man is sitting down ..   a man is sitting in a chair .. \n a man is standing in a field ..   a man is standing on a beach .. \n a man is outside in a park ..   a man is outside at a table .. \n a man is sitting in a park ..   a man is sitting in a restaurant .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a pink shirt is running in his mouth .. \n a young boy is playing with a cat ..   a young boy is sitting on a back .. \n a small boy is running in the snow ..   a small boy is looking at a horse .. \n a man is sitting on his phone ..   a man is sitting in a park .. \n a man is sitting down ..   a man is sitting on a bench .. \n a boy is jumping on a motorcycle in his mouth ..   a boy is jumping into a lake in his mouth .. \n a man is standing in a field ..   a man is standing on a beach .. \n a man is outside in a park ..   a man is outside with a woman .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a green shirt is running in his mouth .. \n a young boy is playing with a cat ..   a young boy is sitting on a back .. \n a small boy is running in the snow ..   a small boy is looking at a horse .. \n a man is sitting down ..   a man is sitting on a bench .. \n a man is standing in a field ..   a man is standing on a beach .. \n a person is getting ready to take a picture ..   a person is getting ready for a how ball .. \n a man is outside in a park ..   a man is outside .. \n a man is sitting in a park ..   a man is sitting on a bench .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a dress is looking at a camera .. \n a young boy is playing with a cat ..   a young boy is sitting on a back .. \n a man is getting ready to go ..   a man is sitting in a church .. \n a small boy is running in the snow ..   a small boy is sitting on a bench .. \n a man is sitting on his phone ..   a man is sitting at a table .. \n a man is sitting down ..   a man is sitting on a bench .. \n a man is standing in a field ..   a man is standing by a building .. \n a man is outside in a park ..   a man is outside .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a young boy is playing with a cat ..   a young boy is sitting on a back .. \n a man is getting ready to go ..   a man is sitting in a church .. \n a small boy is running in the snow ..   a small boy is looking at a horse .. \n a man is sitting on his phone ..   a man is sitting at a table .. \n a man is sitting down ..   a man is sitting in a chair .. \n a boy is jumping on a motorcycle in his mouth ..   a boy is jumping down a sidewalk in his mouth .. \n a man is standing in a field ..   a man is standing outside .. \n a man is outside in a park ..   a man is outside .. \n a kid is playing in a bar ..   a kid is playing with a lot of friends .. \n a man in a white shirt is standing in a restaurant ..   a man in a green shirt is running in his mouth .. \n a young boy is playing with a cat ..   a young boy is sitting on a back .. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option('display.width', 150)\n",
    "pd.options.display.max_rows = 10000\n",
    "for line in df[df['alteration_id'] == 39][['original', 'altered']].iterrows():    \n",
    "    if line[1]['original'] != line[1]['altered']:\n",
    "        print(line[1]['original'], line[1]['altered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Autoreg5stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('Autoreg5stats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
